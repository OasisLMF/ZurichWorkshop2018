{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://oasislmf.org/packages/oasis_theme_package/themes/oasis_theme/assets/src/oasis-lmf-colour.png\" alt=\"Oasis LMF logo\" width=\"250\" align=\"left\"/>\n",
    "<br><br><br>\n",
    "\n",
    "# Excercise 2:   Introduction to Oasis model files and formats.\n",
    "\n",
    "The Oasis kernel provides a robust loss simulation engine for catastrophe modelling. Insurance practitioners are used to dealing with losses arising from events. These losses are numbers, not distributions. Policy terms are applied to the losses individually and then aggregated and further conditions or reinsurances applied. Oasis takes the same perspective, which is to generate individual losses from the probability distributions. The way to achieve this is random sampling called “Monte-Carlo” sampling from the use of random numbers, as if from a roulette wheel, to solve equations that are otherwise intractable.\n",
    "\n",
    "Modelled and empirical intensities and damage responses can show significant uncertainty, Sometimes this uncertainty is multi-modal, meaning that there can be different peaks of behaviour rather than just a single central behaviour. Moreover, the definition of the source insured interest characteristics, such as location or occupancy or construction, can be imprecise. The associated values for event intensities and consequential damages can therefore be varied and their uncertainty can be represented in general as probability distributions rather than point values. The design of Oasis therefore makes no assumptions about the probability distributions and instead treats all probability distributions as probability masses in discrete bins. This includes closed interval point bins such as the values [0,0] for no damage and [1,1] for total damage.\n",
    "\n",
    "The simulation approach taken by the Oasis calculation kernel computes a single cumulative distribution function (CDF) for the damage by “convolving” the binned intensity distribution with the vulnerability matrices. Sampling can then be done against the CDF. \n",
    "\n",
    "<img src=\"images/simulation_methodology.png\" alt=\"Oasis simulation methodology\" width=\"600\"/>\n",
    "\n",
    "The Oasis kernel requires a standard set of files for capturing the hazard footprints and vulnerability data.\n",
    "\n",
    "<img src=\"images/oasis_model_files.png\" alt=\"Oasis model files\" width=\"600\"/>\n",
    "\n",
    "The files are:\n",
    "\n",
    "#### area peril dictionary\n",
    "    The meta-data that describes the model specific geo-spatial grid. This can be a set of points, a regular grid or a variable resolutiuon grid.\n",
    "#### hazard\n",
    "    The hazard values for each impacted area peril cell for each event in the stochastic catalogue.\n",
    "#### event\n",
    "    The list of events in the stochastic catalogue. Event files can be use to distinguish event types, such as historical.\n",
    "#### intensity bin dictionary\n",
    "    The meta-data that descibes the hazard intensities corresponding to the bins.\n",
    "#### vulnerability\n",
    "    The vulnerability data. \n",
    "#### vulnerability dictionary\n",
    "    The meta-data that descibes the vulnerability data, in particular mapping particular curves to particular exposure attributes.\n",
    "#### damage bin dictionary\n",
    "    The meta-data tha descibes the damage percentages corresponding to the bins.\n",
    "\n",
    "\n",
    "## Excercise goals\n",
    "* Understand the Oasis model files\n",
    "* Use Python code to visualise the model files for an example model\n",
    "* Use Python code to visualise run and view the results of an analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import os\n",
    "from shapely.geometry import Point, Polygon\n",
    "from descartes import PolygonPatch\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "import branca.colormap as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_peril_dictionary = pd.read_csv(\"./keys_data/MEEQ/area_peril_dict.csv\")\n",
    "area_peril_dictionary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[\t33.85, 35.50], zoom_start=12, tiles='cartodbpositron')\n",
    "for i, row in area_peril_dictionary.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row.latitude, row.longitude], radius=1).add_to(m)\n",
    "m.fit_bounds(m.get_bounds())\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_bin_dictionary = pd.read_csv(\"./model_data/MEEQ/intensity_bin_dict.csv\")\n",
    "intensity_bin_dictionary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the data contained in the footprint file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "footprints = pd.read_csv(\"./model_data/MEEQ/footprint_subset.csv\")\n",
    "footprints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_peril_dictionary = pd.read_csv(\"./keys_data/MEEQ/area_peril_dict.csv\")\n",
    "area_peril_dictionary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "footprints.event_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "event_id = 3342\n",
    "\n",
    "footprints_with_hazard = footprints.merge(\n",
    "    area_peril_dictionary, how='inner', \n",
    "    on='area_peril_id').merge(\n",
    "    intensity_bin_dictionary, how=\"inner\",\n",
    "    left_on=\"intensity_bin_id\", right_on=\"bin_id\")\n",
    "footprints_with_hazard = footprints_with_hazard[footprints_with_hazard.event_id == event_id]\n",
    "linear = cm.LinearColormap(\n",
    "    ['green', 'yellow', 'red'],\n",
    "    vmin=min(footprints_with_hazard.interpolation), \n",
    "    vmax=max(footprints_with_hazard.interpolation))\n",
    "m = folium.Map(location=[33.85, 35.50], zoom_start=12, tiles='cartodbpositron')\n",
    "for i, row in footprints_with_hazard.iterrows():\n",
    "    c = linear(row.interpolation)\n",
    "    folium.CircleMarker(\n",
    "        location=[row.latitude, row.longitude], fill_color=c, radius=5,\n",
    "        weight=0, fill=True, fill_opacity=1.0).add_to(m)\n",
    "linear.caption = 'log PGA'\n",
    "m.fit_bounds(m.get_bounds())\n",
    "m.add_child(linear)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_bin_dictionary = pd.read_csv(\"./model_data/MEEQ/damage_bin_dict.csv\")\n",
    "damage_bin_dictionary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vulnerabilities = pd.read_csv(\"./model_data/MEEQ/vulnerability_subset.csv\")\n",
    "vulnerabilities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linspace\n",
    "from bokeh.io import output_file, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, FixedTicker, PrintfTickFormatter\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.sampledata.perceptions import probly\n",
    "import colorcet as cc\n",
    "\n",
    "def joy(category, data, scale=1):\n",
    "    return list(zip([category]*len(data), scale*data))\n",
    "\n",
    "num_intensity_bins = len(intensity_bin_dictionary.bin_id)\n",
    "num_damage_bins = len(damage_bin_dictionary.bin_id)\n",
    "\n",
    "cats = list(reversed(probly.keys()))\n",
    "x = linspace(0, 100, num_damage_bins+1)\n",
    "source = ColumnDataSource(data=dict(x=x))\n",
    "intensity_range = list([str(i) for i in intensity_bin_dictionary.bin_id])\n",
    "p = figure(y_range=intensity_range, plot_width=900, x_range=(0, 105), toolbar_location=None)\n",
    "data = vulnerabilities[vulnerabilities.vulnerability_id==712]\n",
    "for i, intensity_bin_id in enumerate(reversed(intensity_range)):        \n",
    "    all_damage_bins=pd.DataFrame({\"damage_bin_id\": list(range(0,num_damage_bins+1))})\n",
    "    d = all_damage_bins.merge(data[data.intensity_bin_id==int(intensity_bin_id)], on=\"damage_bin_id\", how=\"outer\")\n",
    "    d = d.fillna(0)\n",
    "    if len(data[data.intensity_bin_id==int(intensity_bin_id)]) == 1:\n",
    "        d.loc[0, 'probability'] = 0.001\n",
    "        d.loc[2, 'probability'] = 0.001\n",
    "    y = joy(intensity_bin_id, d.probability, 15)\n",
    "    source.add(y, intensity_bin_id)\n",
    "    p.patch('x', intensity_bin_id, alpha=0.6, line_color=\"black\", source=source)\n",
    "\n",
    "p.outline_line_color = None\n",
    "p.background_fill_color = \"#efefef\"\n",
    "\n",
    "p.xaxis.ticker = FixedTicker(ticks=list(range(0, 105, 10)))\n",
    "p.xaxis.formatter = PrintfTickFormatter(format=\"%d%%\")\n",
    "\n",
    "p.ygrid.grid_line_color = None\n",
    "p.xgrid.grid_line_color = \"#dddddd\"\n",
    "p.xgrid.ticker = p.xaxis[0].ticker\n",
    "\n",
    "p.axis.minor_tick_line_color = None\n",
    "p.axis.major_tick_line_color = None\n",
    "p.axis.axis_line_color = None\n",
    "\n",
    "p.y_range.range_padding = 0.12\n",
    "output_notebook()\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the model we need some test exxposure data. Lets have a look at an example Location and Account file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_locations = pd.read_csv('./source_data/MEEQ_loc.csv')\n",
    "test_locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the model, we also need to define some analysis settings. Lets have a look at an example settings file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./analysis_settings.json', 'r') as myfile:\n",
    "    analysis_settings=json.loads(myfile.read().replace('\\n', ''))\n",
    "print(json.dumps(analysis_settings, indent=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! damagebintobin < ./model_data/MEEQ/damage_bin_dict.csv > ./model_data/MEEQ/damage_bin_dict.bin \n",
    "! evetobin < ./model_data/MEEQ/events_subset.csv > ./model_data/MEEQ/events.bin\n",
    "! vulnerabilitytobin -d 52 < ./model_data/MEEQ/vulnerability_subset.csv > ./model_data/MEEQ/vulnerability.bin\n",
    "! footprinttobin -i 30 < ./model_data/MEEQ/footprint_subset.csv\n",
    "! occurrencetobin -P 10000 -D < ./model_data/MEEQ/occurrence.csv > ./model_data/MEEQ/occurrence.bin\n",
    "! returnperiodtobin < ./model_data/MEEQ/returnperiods.csv  > ./model_data/MEEQ/returnperiods.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/analysis_test\n",
    "! oasislmf model run -C oasislmf.json -r /tmp/analysis_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_file\n",
    "from bokeh.models import ColumnDataSource, FactorRange\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import factor_cmap\n",
    "\n",
    "analysis_directory = \"/tmp/analysis_test\"\n",
    "gul_aep = pd.read_csv(os.path.join(analysis_directory, \"output\", \"gul_S1_leccalc_full_uncertainty_aep.csv\"))\n",
    "gul_oep = pd.read_csv(os.path.join(analysis_directory, \"output\", \"gul_S1_leccalc_full_uncertainty_oep.csv\"))\n",
    "eps = pd.merge(gul_oep, gul_aep, on=[\"summary_id\", \"return_period\"], suffixes=[\"_oep\", \"_aep\"])\n",
    "eps = eps.sort_values(by=\"return_period\", ascending=True)\n",
    "return_periods = eps.return_period\n",
    "lec_types = ['OEP', 'AEP']\n",
    "data = {'Return periods' : return_periods,\n",
    "        'OEP': eps.loss_oep,\n",
    "        'AEP': eps.loss_aep}\n",
    "palette = [\"#c9d9d3\", \"#718dbf\"]\n",
    "x = [ (str(return_period), lec_type) for return_period in return_periods for lec_type in lec_types ]\n",
    "counts = sum(zip(data['OEP'], data['AEP']), ())\n",
    "source = ColumnDataSource(data=dict(x=x, counts=counts))\n",
    "p = figure(x_range=FactorRange(*x), plot_height=350, title=\"EP by return period\",\n",
    "           toolbar_location=None, tools=\"\")\n",
    "p.vbar(x='x', top='counts', width=0.9, source=source, line_color=\"white\",\n",
    "       fill_color=factor_cmap('x', palette=palette, factors=lec_types, start=1, end=2))\n",
    "p.y_range.start = 0\n",
    "p.x_range.range_padding = 0.1\n",
    "p.xaxis.major_label_orientation = 1\n",
    "p.xgrid.grid_line_color = None\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ZurichWorkshop2018",
   "language": "python",
   "name": "zurichworkshop2018"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
